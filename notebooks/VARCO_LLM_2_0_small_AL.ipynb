{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy VARCO LLM SMALL 2.0 Algorithm from AWS Marketplace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "VARCO-LLM is NCSOFT’s large language model, which can be applied to develop various NLP-based AI services such as Q&A, chatbot, summarization, information extraction etc. VACRO-LLM, trained with public pre-training data and internally constructed high-quality Korean data, boasts the highest performance among the Korean LLMs of similar sizes that have been released to date (see https://ncsoft.github.io/ncresearch/ for evaluation results). Our models will continue to be updated and we will also release LLMs that support multiple languages or are fined-tuned to specific tasks. As VARCO-LLM is currently in beta service, usage fees will not be charged temporally for this period. For inquiries regarding further performance improvement or collaboration for service applications, please contact us via email (varco_llm@ncsoft.com).\n",
    "\n",
    "This sample notebook shows you how to deploy [varco llm](https://aws.amazon.com/marketplace/pp/prodview-nnewbvmwmt2jy)  using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "2. [Set up environment](#2.-Set-up-environment)\n",
    "3. [Train a model](#3.-Train-a-model)\n",
    "4. [Clean-up](#3.-Clean-up)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the algorithm [listing page](https://aws.amazon.com/marketplace/pp/prodview-nnewbvmwmt2jy)\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algo_arn = \"arn:aws:sagemaker:us-west-2:973735099617:algorithm/finalsmall35\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import numpy as np\n",
    "import io\n",
    "from sagemaker.algorithm import AlgorithmEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "runtime = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "my_instance_type = \"ml.g5.4xlarge\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "my_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_job_name = \"varcosmallTJ\"\n",
    "algo = AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=my_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name=training_job_name,\n",
    "    train_volume_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: varcosmallTJ-2024-04-25-14-13-50-343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-25 14:13:50 Starting - Starting the training job...\n",
      "2024-04-25 14:14:11 Pending - Preparing the instances for training...\n",
      "2024-04-25 14:14:44 Downloading - Downloading input data........................\n",
      "2024-04-25 14:18:44 Downloading - Downloading the training image.....................\n",
      "2024-04-25 14:22:00 Training - Training image download completed. Training in progress........\u001b[34m[W CUDAAllocatorConfig.h:30] Warning: expandable_segments not supported on this platform (function operator())\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.16s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.59s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.09s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.18s/it]\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:145: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mtrainable params: 8,388,608 || all params: 7,088,640,000 || trainable%: 0.11833875045142651\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Dataset length: 333\u001b[0m\n",
      "\u001b[34mWARNING:root:Processing Datasets...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing Datasets...\u001b[0m\n",
      "\u001b[34m#015Map (num_proc=16):   0%|          | 0/333 [00:00<?, ? examples/s]#015Map (num_proc=16):   6%|▋         | 21/333 [00:00<00:02, 121.78 examples/s]#015Map (num_proc=16):  32%|███▏      | 105/333 [00:00<00:00, 397.09 examples/s]#015Map (num_proc=16):  57%|█████▋    | 189/333 [00:00<00:00, 509.60 examples/s]#015Map (num_proc=16):  82%|████████▏ | 273/333 [00:00<00:00, 613.66 examples/s]#015Map (num_proc=16): 100%|██████████| 333/333 [00:00<00:00, 487.14 examples/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.706, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m{'loss': 1.5513, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 436.305, 'train_samples_per_second': 0.763, 'train_steps_per_second': 0.046, 'train_loss': 1.6286221981048583, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/20 [00:00<?, ?it/s]#015  5%|▌         | 1/20 [00:23<07:33, 23.86s/it]#015 10%|█         | 2/20 [00:45<06:42, 22.34s/it]#015 15%|█▌        | 3/20 [01:05<06:06, 21.55s/it]#015 20%|██        | 4/20 [01:26<05:40, 21.29s/it]#015 25%|██▌       | 5/20 [01:46<05:13, 20.91s/it]#015 30%|███       | 6/20 [02:09<05:02, 21.64s/it]#015 35%|███▌      | 7/20 [02:29<04:32, 20.98s/it]#015 40%|████      | 8/20 [02:50<04:10, 20.92s/it]#015 45%|████▌     | 9/20 [03:10<03:47, 20.73s/it]#015 50%|█████     | 10/20 [03:33<03:35, 21.53s/it]#015                                               #015#015 50%|█████     | 10/20 [03:33<03:35, 21.53s/it]#015 55%|█████▌    | 11/20 [03:55<03:13, 21.51s/it]#015 60%|██████    | 12/20 [04:17<02:53, 21.63s/it]#015 65%|██████▌   | 13/20 [04:39<02:33, 21.92s/it]#015 70%|███████   | 14/20 [05:01<02:10, 21.81s/it]#015 75%|███████▌  | 15/20 [05:22<01:48, 21.69s/it]#015 80%|████████  | 16/20 [05:43<01:25, 21.28s/it]#015 85%|████████▌ | 17/20 [06:05<01:04, 21.60s/it]#015 90%|█████████ | 18/20 [06:27<00:43, 21.72s/it]#015 95%|█████████▌| 19/20 [06:50<00:21, 21.94s/it]#015100%|██████████| 20/20 [07:16<00:00, 23.19s/it]#015                                               #015#015100%|██████████| 20/20 [07:16<00:00, 23.19s/it]#015                                               #015#015100%|██████████| 20/20 [07:16<00:00, 23.19s/it]#015100%|██████████| 20/20 [07:16<00:00, 21.81s/it]\u001b[0m\n",
      "\n",
      "2024-04-25 14:31:18 Uploading - Uploading generated training model\n",
      "2024-04-25 14:31:18 Completed - Training job completed\n",
      "Training seconds: 995\n",
      "Billable seconds: 995\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"train\": \"s3://sagemaker-us-west-2-973735099617/finetune/training_data/\"}\n",
    "\n",
    "algo.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.Create an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model package with name: varcosmallTJ-2024-04-25-14-31-43-803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: varcosmallTJ-2024-04-25-14-31-43-803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name varcosmallEP\n",
      "INFO:sagemaker:Creating endpoint with name varcosmallEP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_endpoint_name = \"varcosmallEP\"\n",
    "predictor = algo.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=my_instance_type,\n",
    "    endpoint_name=my_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B.Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"repetition_penalty\": 1.05,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "    \"text\": \"안녕\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C-1. Inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네 안녕하세요"
     ]
    }
   ],
   "source": [
    "class VarcoInferenceStream():\n",
    "    def __init__(self, sagemaker_runtime, endpoint_name):\n",
    "        self.sagemaker_runtime = sagemaker_runtime\n",
    "        self.endpoint_name = endpoint_name\n",
    "\n",
    "    def stream_inference(self, request_body):\n",
    "        # Gets a streaming inference response\n",
    "        # from the specified model endpoint:\n",
    "        response = self.sagemaker_runtime\\\n",
    "            .invoke_endpoint_with_response_stream(\n",
    "                EndpointName=self.endpoint_name,\n",
    "                Body=json.dumps(request_body),\n",
    "                ContentType=\"application/json\"\n",
    "        )\n",
    "        # Gets the EventStream object returned by the SDK:\n",
    "        for body in response[\"Body\"]:\n",
    "            raw = body['PayloadPart']['Bytes']\n",
    "            yield raw.decode()\n",
    "\n",
    "\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "varco_inference_stream = VarcoInferenceStream(sm_runtime, my_endpoint_name)\n",
    "stream = varco_inference_stream.stream_inference(input)\n",
    "for part in stream:\n",
    "    print(part, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
