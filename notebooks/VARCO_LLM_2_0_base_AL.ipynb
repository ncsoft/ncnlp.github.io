{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy VARCO LLM BASE 2.0 Algorithm from AWS Marketplace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "VARCO-LLM is NCSOFT’s large language model, which can be applied to develop various NLP-based AI services such as Q&A, chatbot, summarization, information extraction etc. VACRO-LLM, trained with public pre-training data and internally constructed high-quality Korean data, boasts the highest performance among the Korean LLMs of similar sizes that have been released to date (see https://ncsoft.github.io/ncresearch/ for evaluation results). Our models will continue to be updated and we will also release LLMs that support multiple languages or are fined-tuned to specific tasks. As VARCO-LLM is currently in beta service, usage fees will not be charged temporally for this period. For inquiries regarding further performance improvement or collaboration for service applications, please contact us via email (varco_llm@ncsoft.com).\n",
    "\n",
    "This sample notebook shows you how to deploy [varco llm](https://aws.amazon.com/marketplace/pp/prodview-nnewbvmwmt2jy)  using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "2. [Set up environment](#2.-Set-up-environment)\n",
    "3. [Train a model](#3.-Train-a-model)\n",
    "4. [Clean-up](#3.-Clean-up)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the algorithm [listing page](https://aws.amazon.com/marketplace/pp/prodview-nnewbvmwmt2jy)\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algo_arn = \"arn:aws:sagemaker:us-west-2:973735099617:algorithm/finalsmall35\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import numpy as np\n",
    "import io\n",
    "from sagemaker.algorithm import AlgorithmEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "runtime = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "my_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "my_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_job_name = \"varcobaseTJ\"\n",
    "algo = AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=my_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name=training_job_name,\n",
    "    train_volume_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: varcobaseTJ-2024-04-25-04-04-24-147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-25 04:04:24 Starting - Starting the training job...\n",
      "2024-04-25 04:04:30 Pending - Training job waiting for capacity...\n",
      "2024-04-25 04:04:56 Pending - Preparing the instances for training...\n",
      "2024-04-25 04:05:41 Downloading - Downloading input data............................................................\n",
      "2024-04-25 04:15:33 Training - Training image download completed. Training in progress.\u001b[34m#015Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]#015Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.48s/it]#015Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.43s/it]#015Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.42s/it]#015Loading checkpoint shards:  67%|██████▋   | 4/6 [00:09<00:04,  2.42s/it]#015Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.41s/it]#015Loading checkpoint shards: 100%|██████████| 6/6 [00:12<00:00,  1.81s/it]#015Loading checkpoint shards: 100%|██████████| 6/6 [00:12<00:00,  2.12s/it]\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:145: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Dataset length: 333\u001b[0m\n",
      "\u001b[34mWARNING:root:Processing Datasets...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing Datasets...\u001b[0m\n",
      "\u001b[34mtrainable params: 1,638,400 || all params: 13,193,139,200 || trainable%: 0.01241857586100509\u001b[0m\n",
      "\u001b[34m#015Map (num_proc=32):   0%|          | 0/333 [00:00<?, ? examples/s]#015Map (num_proc=32):   3%|▎         | 10/333 [00:01<00:36,  8.85 examples/s]#015Map (num_proc=32):   6%|▌         | 19/333 [00:02<00:35,  8.93 examples/s]#015Map (num_proc=32):  10%|▉         | 33/333 [00:03<00:26, 11.29 examples/s]#015Map (num_proc=32):  12%|█▏        | 41/333 [00:04<00:28, 10.07 examples/s]#015Map (num_proc=32):  16%|█▌        | 54/333 [00:05<00:25, 11.15 examples/s]#015Map (num_proc=32):  20%|█▉        | 65/333 [00:06<00:24, 11.15 examples/s]#015Map (num_proc=32):  22%|██▏       | 74/333 [00:07<00:24, 10.47 examples/s]#015Map (num_proc=32):  26%|██▋       | 88/333 [00:08<00:21, 11.63 examples/s]#015Map (num_proc=32):  30%|██▉       | 99/333 [00:09<00:20, 11.48 examples/s]#015Map (num_proc=32):  33%|███▎      | 110/333 [00:10<00:19, 11.41 examples/s]#015Map (num_proc=32):  36%|███▌      | 120/333 [00:11<00:19, 10.98 examples/s]#015Map (num_proc=32):  39%|███▊      | 129/333 [00:12<00:19, 10.41 examples/s]#015Map (num_proc=32):  43%|████▎     | 143/333 [00:12<00:16, 11.56 examples/s]#015Map (num_proc=32):  46%|████▌     | 153/333 [00:13<00:16, 11.12 examples/s]#015Map (num_proc=32):  49%|████▉     | 163/333 [00:14<00:15, 10.78 examples/s]#015Map (num_proc=32):  52%|█████▏    | 173/333 [00:15<00:15, 10.61 examples/s]#015Map (num_proc=32):  55%|█████▍    | 182/333 [00:16<00:14, 10.17 examples/s]#015Map (num_proc=32):  57%|█████▋    | 191/333 [00:17<00:14,  9.85 examples/s]#015Map (num_proc=32):  61%|██████    | 203/333 [00:18<00:12, 10.55 examples/s]#015Map (num_proc=32):  64%|██████▍   | 213/333 [00:19<00:11, 10.38 examples/s]#015Map (num_proc=32):  66%|██████▌   | 219/333 [00:20<00:12,  9.06 examples/s]#015Map (num_proc=32):  70%|██████▉   | 233/333 [00:21<00:09, 10.61 examples/s]#015Map (num_proc=32):  73%|███████▎  | 243/333 [00:22<00:08, 10.35 examples/s]#015Map (num_proc=32):  76%|███████▌  | 253/333 [00:23<00:07, 10.35 examples/s]#015Map (num_proc=32):  79%|███████▉  | 263/333 [00:24<00:06, 10.23 examples/s]#015Map (num_proc=32):  82%|████████▏ | 273/333 [00:25<00:05, 10.19 examples/s]#015Map (num_proc=32):  84%|████████▍ | 281/333 [00:26<00:05,  9.55 examples/s]#015Map (num_proc=32):  87%|████████▋ | 290/333 [00:27<00:04,  9.39 examples/s]#015Map (num_proc=32):  91%|█████████ | 303/333 [00:28<00:02, 10.53 examples/s]#015Map (num_proc=32):  94%|█████████▍| 313/333 [00:29<00:01, 10.38 examples/s]#015Map (num_proc=32):  97%|█████████▋| 323/333 [00:30<00:00, 10.30 examples/s]#015Map (num_proc=32): 100%|██████████| 333/333 [00:31<00:00, 10.30 examples/s]#015Map (num_proc=32): 100%|██████████| 333/333 [00:31<00:00, 10.44 examples/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/20 [00:00<?, ?it/s]#015  5%|▌         | 1/20 [00:35<11:10, 35.27s/it]#015 10%|█         | 2/20 [01:06<09:53, 32.97s/it]#015 15%|█▌        | 3/20 [01:37<09:01, 31.84s/it]#015 20%|██        | 4/20 [02:07<08:21, 31.35s/it]#015 25%|██▌       | 5/20 [02:37<07:41, 30.74s/it]#015 30%|███       | 6/20 [03:12<07:29, 32.12s/it]#015 35%|███▌      | 7/20 [03:40<06:42, 30.97s/it]#015 40%|████      | 8/20 [04:11<06:09, 30.79s/it]#015 45%|████▌     | 9/20 [04:41<05:35, 30.54s/it]#015 50%|█████     | 10/20 [05:15<05:18, 31.82s/it]#015                                               #015#015 50%|█████     | 10/20 [05:15<05:18, 31.82s/it]#015 55%|█████▌    | 11/20 [05:47<04:45, 31.76s/it]#015 60%|██████    | 12/20 [06:20<04:16, 32.02s/it]#015 65%|██████▌   | 13/20 [06:53<03:48, 32.58s/it]#015 70%|███████   | 14/20 [07:25<03:14, 32.34s/it]#015 75%|███████▌  | 15/20 [07:57<02:40, 32.07s/it]#015 80%|████████  | 16/20 [08:27<02:05, 31.47s/it]#015 85%|████████▌ | 17/20 [09:00<01:35, 31.96s/it]#015 90%|█████████ | 18/20 [09:33<01:04, 32.21s/it]#015 95%|█████████▌| 19/20 [10:06<00:32, 32.54s/it]#015100%|██████████| 20/20 [10:45<00:00, 34.48s/it]#015                                               #015#015100%|██████████| 20/20 [10:45<00:00, 34.48s/it]#015                                               #015#015100%|██████████| 20/20 [10:45<00:00, 34.48s/it]#015100%|██████████| 20/20 [10:45<00:00, 32.28s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.686, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m{'loss': 1.495, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 645.6676, 'train_samples_per_second': 0.516, 'train_steps_per_second': 0.031, 'train_loss': 1.5905011653900147, 'epoch': 0.96}\u001b[0m\n",
      "\n",
      "2024-04-25 04:27:36 Uploading - Uploading generated training model\n",
      "2024-04-25 04:27:46 Completed - Training job completed\n",
      "Training seconds: 1325\n",
      "Billable seconds: 1325\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"train\": \"s3://sagemaker-us-west-2-973735099617/finetune/training_data/\"}\n",
    "\n",
    "algo.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.Create an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model package with name: varcobaseTJ-2024-04-25-04-39-25-707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: varcobaseTJ-2024-04-25-04-39-25-707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name varcobaseEP\n",
      "INFO:sagemaker:Creating endpoint with name varcobaseEP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------!"
     ]
    }
   ],
   "source": [
    "my_endpoint_name = \"varcobaseEP\"\n",
    "predictor = algo.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=my_instance_type,\n",
    "    endpoint_name=my_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B.Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"repetition_penalty\": 1.05,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "    \"text\": \"안녕\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-1. Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpointWithResponseStream operation: Endpoint varcobaseEP of account 973735099617 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m varco_inference_stream \u001b[38;5;241m=\u001b[39m VarcoInferenceStream(sm_runtime, my_endpoint_name)\n\u001b[1;32m     23\u001b[0m stream \u001b[38;5;241m=\u001b[39m varco_inference_stream\u001b[38;5;241m.\u001b[39mstream_inference(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(part, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m, in \u001b[0;36mVarcoInferenceStream.stream_inference\u001b[0;34m(self, request_body)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, request_body):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Gets a streaming inference response\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# from the specified model endpoint:\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint_with_response_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_body\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Gets the EventStream object returned by the SDK:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m body \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpointWithResponseStream operation: Endpoint varcobaseEP of account 973735099617 not found."
     ]
    }
   ],
   "source": [
    "class VarcoInferenceStream():\n",
    "    def __init__(self, sagemaker_runtime, endpoint_name):\n",
    "        self.sagemaker_runtime = sagemaker_runtime\n",
    "        self.endpoint_name = endpoint_name\n",
    "\n",
    "    def stream_inference(self, request_body):\n",
    "        # Gets a streaming inference response\n",
    "        # from the specified model endpoint:\n",
    "        response = self.sagemaker_runtime\\\n",
    "            .invoke_endpoint_with_response_stream(\n",
    "                EndpointName=self.endpoint_name,\n",
    "                Body=json.dumps(request_body),\n",
    "                ContentType=\"application/json\"\n",
    "        )\n",
    "        # Gets the EventStream object returned by the SDK:\n",
    "        for body in response[\"Body\"]:\n",
    "            raw = body['PayloadPart']['Bytes']\n",
    "            yield raw.decode()\n",
    "\n",
    "\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "varco_inference_stream = VarcoInferenceStream(sm_runtime, my_endpoint_name)\n",
    "stream = varco_inference_stream.stream_inference(input)\n",
    "for part in stream:\n",
    "    print(part, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: varcobaseTJ-2024-04-25-04-39-25-707\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: varcobaseEP\n",
      "INFO:sagemaker:Deleting endpoint with name: varcobaseEP\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
